1. 
For each of parts (a) through (d), indicate whether we would generally
expect the performance of a flexible statistical learning method to be
better or worse than an inflexible method. Justify your answer.

(a) The sample size n is extremely large, and the number of predictors
p is small.
Better - the model will take advantage of large number of observations

(b) The number of predictors p is extremely large, and the number
of observations n is small.
Worse - due to small sample size

(c) The relationship between the predictors and response is highly
non-linear.
Better - flexible is better than rigid to follow non linear relationships

(d) The variance of the error terms, i.e. σ^2 = Var(), is extremely
high.

To have high σ^2 means sample size is small and standard dev σ between yhat and Y is large
Hence, a flexible model would be worse

In other words, since the standard error (Var.u = SE.u^2 = σ^2/n) tells the 
average amount that the estimate of u (uhat) [the sample mean of the error]
differs from the true u (the mean of the error), the bias between yi and Y would be large i.e. a lot of the
values will be far from true Y so the noise woudl be prevalent.

Hence, a flexible model would cause to fit too much of the noise.




2. 
Explain whether each scenario is a classification or regression problem,
and indicate whether we are most interested in inference or prediction.
Finally, provide n and p.

(a) We collect a set of data on the top 500 firms in the US. For each
firm we record profit, number of employees, industry and the
CEO salary. We are interested in understanding which factors
affect CEO salary.
Regresson and inference (salary from params)
p = 3 (industry, employee number and profit)
n = 500

(b) We are considering launching a new product and wish to know
whether it will be a success or a failure. We collect data on 20
similar products that were previously launched. For each product
we have recorded whether it was a success or failure, price
charged for the product, marketing budget, competition price,
and ten other variables.
Classification and prediction
n = 20
p = 13

(c) We are interesting in predicting the % change in the US dollar in
relation to the weekly changes in the world stock markets. Hence
we collect weekly data for all of 2012. For each week we record
the % change in the dollar, the % change in the US market,
the % change in the British market, and the % change in the
German market.
Regression and prediction
n = 52 (weeks in a year)
p = 4



3. 
We now revisit the bias-variance decomposition.

Explain why each of the five curves has the shape displayed in part (a).

a. Squared bias decreases as we increase flexibility as a flexible func is closer to observed data
(line closer to points)

b. Variance increases because more flexible methods fllow the points more closely.
e.g.. using a different training data set or data set will create higher var in modeled fhat
with a high number of p (flexible) because different points will be followed closely

c. var(e) is the irreducible error. The irreducible error contains variables that are not measured, 
so they can't be used for the prediction. By definition e cant be predicted using X.
It correspond to the lowest possible achievable test MSE (var(e) and sq bias sum near 0)


c. test MSE is the sum of the above three. It initially decreases until it increases again, 
as variance increases and squared bias decreases. The relationship is reffered to as the bias-variance trade off

d. The training MSE declines monotonically as flexibility increases, this is because as flexibility increases the f
curve fits the observed data more closely. This cause overfitting if the model is created to minimise train MSE, i.e.
in reality the supposed pattern does not exist, so the test data MSE will not follow prediction of the curve for the model based onto too flexible train data model
